\documentclass[12pt]{article}

% report, book

%  Русский язык

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} 
\usepackage{float}

\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[table,xcdraw]{xcolor}

\usepackage{wasysym}

\usepackage{geometry} 
\geometry{a4paper,top=2cm,bottom=3cm,left=2cm}

\begin{document}

\begin{titlepage}

\thispagestyle{empty}

\centerline{НИУ ИТМО}
\centerline{Факультет Информационных Технологий и Программирования}
\centerline{Направление "Прикладная Математика и Информатика"}

\vfill

\centerline{\huge{Лабораторная работа 4}}
\centerline{\large{курса ``Методы оптимизации'' }}
\vspace{1cm} 
\centerline{\large{Верблюжий случай}}
\centerline{\large{Сысоев Александр, Зырянова Мария}}
\vfill

\centerline{Санкт-Петербург, 2021}
\clearpage
\end{titlepage}

\section{Постановка задания}

Необходимо разработать и исследовать программы для безусловной минимизации функции многих переменных методом Ньютона и его модификаций.

\begin{enumerate}
	\item Реализовать классический метод Ньютона, с одномерным поиском и с направлением спуска. 
		\begin{enumerate}
			\item Продемонстрировать работу на различных функциях, включая иллюстрации траекторий спуска. Указать количество итераций, необходимых для достижения заданной точности; указывать найденные значения параметров при одномерном поиске.
			\item Исследовать работу на двух функциях с заданным начальным приближением. Сравнить резултаты с минимизацией методом наискорейшего спуска из лабораторной работы 2. Для каждого метода привести иллюстрации траекторий сходимости.
		\end{enumerate}				
	\item Реализовать метод Бройдена-Флетчера-Шено и метод Пауэлла. Работу квазиньютоновских методов сравните с наилучшим методом Ньютона
(по результатам 1.2) на заданных функциях. Провести исследование влияние выбора начального приближения на результат (не менее трех) и оценить скорость сходимости.
	\item Реализовать метод Марквардта двумя вариантами. Результаты работы продемонстрировать на минимизации многомерной функции
Розенброка в сравнении с наилучшим методом Ньютона.
\end{enumerate}

\newpage
\section{Рассматриваемые функции}

\begin{enumerate}
 	\item \[f_1(x) = 50x^2+y^2+20x+20y+239\]
 	Минимум достигается в точке $x_{min} = (-0,2; -10)$, значение минимума -- 137.
 	
 	\item \[f_2 = (x-2)^4+(y-3)^4+(z-4)^4\]
 	Минимум достигается в точке $x_{min} = (2; 3; 4)$, значение минимума -- 0.
 	
 	\item \[f_3 = x_1^2+x_2^2-1.2x_1x_2\]
 	Минимум достигается в точке $x_{min} = (0; 0)$, значение минимума -- 0.
 	
 	\item \[f_4 = 100 (x_2-x_1^2)^2+(1-x_1)^2\]
 	Минимум достигается в точке $x_{min} = (1; -1)$ и $x_{min} = (1; 1)$, значение минимума -- 0.
 	
 	\item \[ f_5 = (x_1^2+x_2-11)^2+(x_1+x_2^2-7)^2 \] 
 	Минимум достигается в 
 	\begin{itemize}
 		\item $x_{min} = (3; 2)$;
 		\item $x_{min} = (-3.779310253377746891890765841; -3.283185991286169412266000514)$
 		\item $x_{min} = (-2.805118086952744853053572398; 3.131312518250572965804300723)$
 		\item $x_{min} = (3.584428340330491744944338239; -1.848126526964403553538300209)$
 	\end{itemize} 
 	Значение минимума во всех случаях 0.
 	 	
 	\item \[ f_6 = (x_1+10x_2)^2+5(x_3-x_4)^2+(x_2-2x_3)64+10(x_1-x_4)^4 \]
 	Минимум достигается в точке давайте расскажите как мне тут минимум посчитать.
 	
 	\item \[ f_7 = 100 - \frac{2}{1+\left( \frac{x_1-1}{2} \right) ^2 + \left( \frac{x_2-1}{3} \right) ^2} - \frac{1}{1+\left( \frac{x_1-2}{2} \right) ^2 + \left( \frac{x_2-1}{3} \right) ^2}\] 
 	Минимум достигается в точке $x_{min} = (1.291643031517492930227373527; 1)$, значение минимума -- 97.1531028728543166387604846.
 	
 	\item Функция Розенброка:
 	\[ f = \sum_1^{n-1} 100(x_{i+1}-x_i^2)^2+(1-x_i)^2 \]
 	Минимум ищите сами
\end{enumerate}



\newpage
\section{Метод Ньютона и его модификации}

Метод минимизации Ньютона основывается на следующих рассуждениях.

Пусть дана дважды дифференцируемая целевая функция нескольких переменных $f(x)$, которую необходимо минимизировать. Разложим ее в ряд Тейлора в фиксиованной точке $x$ при произвольном приращении аргумента $\Delta x$:

\[ f(x+\Delta x) = f(x) + \nabla f(x)^T \Delta x + \frac{1}{2} \Delta x^T \nabla^2f(x)\Delta x + o(\lVert x \rVert ^ 2) \]

Пусть $s = \Delta x, \; g(x) = \nabla f(x), \; H(x) = \nabla^2 f(x)$, принебрежем слагаемыми второго порядка малости, получим квадратичную функцию:

\[ q(x) = f(x) + g(x)^Ts + \frac{1}{2} s^T H(x)s \]

Полученная функция достигает минимума при $s: \; H(x)s = -g(x)$, так как необходимое условие минимума -- $\nabla q(x) = 0$:

\[ \nabla q(x) = g(x)+H(x)s \] 

Тогда вектор перемещения в точку минимума квадратичной функции

\[ s = -H(x)^{-1} g(x) \]

Метод Ньютона минимизирует положительно определенную квадратичную функцию за один шаг из любой начальной точки $x_0$. Это метод второго порядка, так как используется вычисление гессиана -- вторые частные производные.

\subsection{Классический метод Ньютона}

Классический метод Ньютона является итерационным. Обозначая в текущей точке поиска $x_k$ значения градиента $g_k = \nabla f(x_k)$ и матрицы Гессе $H_k = \nabla^2 f(x_k)$ получаются следующие итерационные формулы метода Ньютона:

\[ x_{k+1}=x_k+s_k, \quad H_k s_k = -g_k \]\

Итерационный процесс продолжается по мере выполнения условия прерывания с заданной допустимой погрешностью $\epsilon$: $\lVert x_{k+1} - x_k \rVert > \epsilon$

Вектор $s$ находится за счет решения системы линейных уравнений LU-разложением, реализация которого была подробно рассмотрена в 3 лабораторной работе. Этот вектор задает и направление, и шаг перехода в следующую точку поиска.

\subsection{Метод Ньютона с одномерным поиском}

Формулы метода Ньютона основаны на аппроксимации функции квадратичной функцией в малой окрестности текущей точки поиска и могут приводить к возрастанию значений функции. Для улучшения применяется одномерная минимизация функции в направлении поиска $s$.

На каждой итерации такого модифицированного метода Ньютона из точки $x_k$ выполняется одномерный поиск в направлении метода Ньютона $d_k = s_k$:

\[ \lambda_k = arg min f(x_k+\lambda d_k) \]

Таким образом, итерационный процесс дополняется следующими формулами:

\[ H_k d_k = -g_k, \quad r = arg min f(x_k+\lambda d_k), \quad s = r \cdot d \]

Метод Ньютона с одномерным поиском надежнее исходного метода Ньютона за счет обеспечения убывания значений целевой функции. Однако его эффективность существенно зависит от того, является ли направление поиска направлением спуска.

\subsection{Метод Ньютона с направлением спуска}

Гарантией убывания значения функции является нахождение такого направления одномерного поиска, которое будет и направлением спуска. Метод Ньютона приводит к ошибочному направлению в точке максимума и в седловых точках. 

$s_k$ -- направление спуска, если скалярное произведение вектора направления из точки $x_k$ и вектора градиента в этой точке отрицательно (то есть между ними образуется тупой угол). Если же угол острый, то $s_k$ не является направлением убывания, следовательно, разумнее использовать направление антиградиента, так как оно гарантирует убывание функции.

Таким образом, направление спуска задается следующим образом:

\[ H_k s_k = -g_k \]
\[ d_k = 
	\begin{cases}
		s_k, \quad s_k^t g_k < 0 \\
		-g_k, \quad s_k^t g_k \geq 0
	\end{cases}
\]

Накопленный опыт показал, что первую итерацию разумнее проводить в направлении антиградиента с использованием одномерного поиска. Тогда на первом шаге $d_0 = -g_0, \; r_0 = arg min f(x_0+\lambda d_0), \; s_0 = r_0 \cdot d_0, \; x_1 = x_0 + s_0$. Далее выполняется итерационный процесс, пока $\lVert s_k \rVert > \epsilon$:

\begin{enumerate}
 	\item $Hs = -g$
 	\item Если $s^T \cdot g < 0$, то $d = s$, иначе $d = -g$
 	\item $r = arg min f(x+\lambda d), \; s = rd$
 	\item $x = x + s$
\end{enumerate}

\newpage
\section{Квазиньютоновские методы}

Квазиньютоновские методы основываются на методе Ньютона с одномерным поиском. В методах Ньютона требуется вычисление гессиана, что при больших размерностях пространства требует больших затрат машинного времени, поэтому разумно аппроксимировать матрицу Гессе или обратную к ней с использованием значений градиента. В итоге, получается метод первого порядка.

Гессиан -- симметрическая матрица, поэтому и обратная к ней $G(x)$ тоже симметрическая. Зададим ее начальное приближение в виде некоторой симметрической положительно определенной матрицы $G_0$. Такое задание обеспечивает соответствующее направление одномерного поиска $d_0=-G_0g_0$ из начальной точки $x_0$ как направление спуска. Следующая точка ищется так:

\[ x_1 = x_0+\lambda_0 d_0, \; \text{где} \; \lambda_0=arg min f(x_0+\lambda d_0) \]

В последующих итерациях построение аппроксимирующей матрицы основано на свойстве квадратичной функции:

\[ g_{k+1}-g_k = H(x_{k+1}-x_k) \]

Пусть $p_k=g_{k+1}-g_k, \; s_k = x_{k+1}-x_k$, тогда $Gp_k=s_k$. Если известна аппроксимация для $G$, то следующее ее приближение в точке $x_{k+1}$ должно удовлетворять уравнению 

\[ G_{k+1}p_k = s_k \]

Но найти такую матрицу невозможно, так как количество неизвестных в силу симметричности $G$ превышает количество уравнений $n$. Поэтому используется дополнительное условие -- уравнение коррекции:

\[ G_{k+1} = G_k + \Delta G_k \]

Различные квазиньютоновские методы отличаются между собой формулами для поправки $\Delta G_k$.

\subsection{Метод Бройдена-Флетчера-Шено}

В данном методе аппроксимируется сама матрица Гессе по принципу: $H_{k+1}s_k = p_k$. Для нее поправка имеет вид:

\[ \frac{p_k p_k^T}{p_k^t s_k} - \frac{H_k s_k (H_k s_k)^T}{(H_k s_k)^T s_k} \]

Поскольку аппроксимируется гессиан, а не обратная матрица, поэтому направление поиска ищется путем решения СЛАУ: $H_k d_k = -g_k$.

\subsection{Метод Пауэлла}

\[ \Delta G_k = - \frac{u \cdot u^T}{\left(u^T, p \right)} \]

где $u_k = s_k - G_kp, \; p = \nabla f(x_{k+1}) - \nabla f(x_{k})$.

Направление одномерного поиска $d_k = -Gg_k$.

\newpage
\section{Метод Марквардта}

Этот метод является комбинацией методов наискорейшего спуска и метода Ньютона. Движение в направлении антиградиента из начальной точки поиска обычно приводит к существенному уменьшению целевой функции. С другой стороны, направления эффективного поиска в окрестности точки минимума определяются по методу Ньютона.

Этот метод и его модификация характеризуются относительной простотой, свойством убывания $f(x)$ при переходе от итерации к итерации, высокой скоростью сходимости в окрестности $x^*$ и отсутствием процедуры одномерного поиска.

\subsection{Обычный метод Марквардта}

В данном методе используется следующая идея ($\alpha$ -- скалярный параметр, $E$ -- единичная матрица):

\[ \left( H(x) + \alpha E \right) s = -g(x) \]

\begin{enumerate}
	\item При большом значении $\alpha$ матрицей $H$ можно пренебречь, тогда получается уравнение $\alpha Es = -g(x)$, и в этом случае направление вектора $s$ совпадает с антиградиентом (направление наискорейшего спуска).
	\item При $\alpha \rightarrow 0$ можно пренебречь $\alpha E$, тогда $s$ -- шаг метода Ньютона.
\end{enumerate} 

Таким образом, выбрав начальное значение $\alpha_0$ достаточно большим, а затем уменьшая его, сначала будут выполняться шаги наискорейшего спуска, а на конечных этапах -- метода Ньютона. 

В общем виде:

\[ x_{k+1} = x_k + s_k, \; \left( H_k + \alpha_k E \right) s_k = -g_k, \; \alpha_k = \alpha{k-1} \beta \].

Но если значение функции в новой точке больше, чем в предыдушей, то требуется корректировка $\alpha := \frac{\alpha}{\beta}$.

\subsection{Метод Марквардта с применением разложения Холецкого}

Можно начинать с $\alpha_0 = 0$, тогда на каждой итерации должно выполняться условие на $ H+\alpha E$ -- она должна быть положительно определенная. Если это условие не выполняется, то перевыбирается $\alpha = max \left( 1, 2\alpha \right)$.

Условие положительно определенной формы проверяется с помощью разложения Холецкого: $H+\alpha I = LL^T$, где $L$ -- нижнетреугольная матрица. Тогда если такое разложение возможно, то она положительно определена.

Сложность метода в таком случае будет складываться из числа запусков алгоритма Холецкого. Его сложность -- $O(n^3)$

\newpage
\section{Выводы}

Метод Марквардта позволяет устранить недостатки метода Ньютона -- невозможность корректировки длины шага и возможно плохую обсуловленность СЛАУ.




Реализация: \href{https://github.com/Mr3zee/Linear-Equations}{GitHub}.

\end{document}